# BÃºsqueda de Arquitecturas Neuronales con Aprendizaje por Refuerzo para CIFAR-10

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

ImplementaciÃ³n completa de **Neural Architecture Search (NAS)** utilizando el algoritmo **REINFORCE** para descubrir automÃ¡ticamente arquitecturas CNN Ã³ptimas para la clasificaciÃ³n de CIFAR-10. Este proyecto migra el cÃ³digo original en TensorFlow 1.x a PyTorch moderno, incorporando mejoras orientadas a investigaciÃ³n y producciÃ³n.

## ğŸ¯ Resumen del Proyecto

El sistema NAS implementado utiliza una red recurrente (Controlador LSTM) para generar arquitecturas neuronales y la entrena con aprendizaje por refuerzo para maximizar la exactitud de validaciÃ³n. El sistema se valida en CIFAR-10 e incluye la arquitectura NASCNN15 descubierta mediante este proceso.

### Funcionalidades Clave

- âœ… **NAS con REINFORCE** para generar arquitecturas
- âœ… **Controlador LSTM** que produce secuencias de ADN arquitectÃ³nico
- âœ… **Constructor dinÃ¡mico de CNN** a partir de ADN
- âœ… **Pipeline de entrenamiento** con early stopping y scheduler de LR
- âœ… **Checkpoints** con capacidad de reanudaciÃ³n
- âœ… **Logging narrativo** para documentaciÃ³n de investigaciones
- âœ… **CLI con dos modos** (bÃºsqueda NAS + entrenamiento NASCNN15)
- âœ… **CÃ³digo listo para producciÃ³n** con pruebas exhaustivas (histÃ³ricas)

## ğŸ“š NAS Explicado

### Â¿QuÃ© es NAS?

**Neural Architecture Search** es una tÃ©cnica de AutoML que explora automÃ¡ticamente el espacio de arquitecturas para encontrar redes neuronales de alto desempeÃ±o sin intervenciÃ³n manual.

### Â¿CÃ³mo funciona?

Este proyecto sigue la propuesta de [&#34;Neural Architecture Search with Reinforcement Learning&#34;](https://arxiv.org/abs/1611.01578) (Zoph & Le, 2017):

![NAS Architecture Overview](https://miro.medium.com/max/656/1*hIif88uJ7Te8MJEhm40rbw.png)

**Flujo del proceso:**

1. **Controlador (LSTM)** genera descripciones de arquitectura (ADN)
2. **ADN** codifica la estructura de la red: `[kernel, filtros, stride, pool, ...]`
3. **Red hija** se construye dinÃ¡micamente y se entrena en CIFAR-10
4. **Exactitud de validaciÃ³n** se usa como recompensa
5. **REINFORCE** actualiza el Controlador para mejorar futuras arquitecturas

![NAS Training Process](https://i.ytimg.com/vi/CYUpDogeIL0/maxresdefault.jpg)

### CodificaciÃ³n ADN

```python
# ADN para una CNN de 3 capas:
DNA = [
    [3, 64,  1, 1],   # Capa 1: kernel 3x3, 64 filtros, stride=1, sin pooling
    [5, 128, 1, 2],   # Capa 2: kernel 5x5, 128 filtros, stride=1, pooling 2x2
    [3, 256, 1, 1]    # Capa 3: kernel 3x3, 256 filtros, stride=1, sin pooling
]
```

**Componentes:**

- **TamaÃ±o de kernel:** 1-7
- **NÃºmero de filtros:** 32-512
- **Stride:** 1-2
- **Pool:** 1-3 (1 = sin pooling)

### Algoritmo REINFORCE

El Controlador se entrena con gradiente de polÃ­ticas:

```
âˆ‡J(Î¸) = E[R Ã— âˆ‡log P(a|Î¸)]
```

donde **R** es la recompensa (exactitud de validaciÃ³n).

### Arquitectura NASCNN15

Arquitectura de 15 capas descubierta para CIFAR-10:

```
Entrada (3Ã—32Ã—32)
  â†“
Conv 1: 3Ã—3, 36 filtros
  â†“
Conv 2: 3Ã—3, 48 filtros
  â†“
...
Conv 15: 7Ã—5, 48 filtros
  â†“
Global Average Pooling
  â†“
Capa totalmente conectada (10 clases)
```

**CaracterÃ­sticas:**

- Sin stride ni pooling (resoluciÃ³n fija 32Ã—32)
- Conexiones densas por concatenaciÃ³n
- Kernels variados (1Ã—1 a 7Ã—7)
- BatchNorm + ReLU tras cada convoluciÃ³n

## ğŸš€ Inicio RÃ¡pido

### InstalaciÃ³n

1. Clonar el repositorio:
   ```bash
   git clone https://github.com/tu-usuario/VC-ARN.git
   cd VC-ARN
   ```
2. Crear entorno virtual:
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # Windows: .venv\Scripts\activate
   ```
3. Instalar dependencias:
   ```bash
   pip install -r requirements.txt
   ```

### CLI (dos modos)

#### Modo 1: BÃºsqueda NAS

- **Prueba rÃ¡pida (5-10 min):**
  ```bash
  cd app
  python main.py --mode nas --config fast --episodes 5 --children 2
  ```
- **BÃºsqueda corta (experimental):**
  ```bash
  python main.py --mode nas --config fast --episodes 50 --children 5
  ```
- **BÃºsqueda completa (producciÃ³n):**
  ```bash
  python main.py --mode nas --config default
  ```
- **Reanudar desde checkpoint:**
  ```bash
  python main.py --mode nas --resume checkpoints/nas/nas_episode_50.pth
  ```

#### Modo 2: Entrenar NASCNN15

```bash
python main.py --mode train
```

#### Ayuda del CLI

```bash
python main.py --help
```

### Configuraciones

| ConfiguraciÃ³n | Episodios | Hijos/episodio | Ã‰pocas por hijo | Arquitecturas totales | Tiempo aprox. |
| -------------- | --------- | -------------- | ---------------- | --------------------- | ------------- |
| `fast`       | 100       | 5              | 20               | 500                   | 2-3 horas     |
| `default`    | 2,000     | 10             | 100              | 20,000                | 100-150 horas |
| `thorough`   | 5,000     | 15             | 150              | 75,000                | 500-600 horas |

## ğŸ“ Estructura del Proyecto

```
VC-ARN/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                          # CLI (NAS + entrenamiento)
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ arqui_cnn.py                 # Arquitectura NASCNN15
â”‚       â”œâ”€â”€ load.py                      # Carga y split de CIFAR-10
â”‚       â”œâ”€â”€ train_pipeline.py            # Orquestador de entrenamiento
â”‚       â”œâ”€â”€ auxiliares.py                # Funciones auxiliares
â”‚       â”œâ”€â”€ pre_processed.py             # Utilidades de preprocesamiento
â”‚       â””â”€â”€ nas/                         # MÃ³dulo NAS
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ configs.py               # Configuraciones NAS
â”‚           â”œâ”€â”€ utils.py                 # Utilidades ADN
â”‚           â”œâ”€â”€ controller.py            # Controlador LSTM
â”‚           â”œâ”€â”€ child_builder.py         # Constructor de CNN
â”‚           â”œâ”€â”€ reinforce.py             # Optimizador REINFORCE
â”‚           â””â”€â”€ trainer.py               # Orquestador NAS
â”‚
â”œâ”€â”€ datasets/                            # Cache de CIFAR-10 (generado)
â”œâ”€â”€ experiments/                         # Salidas y checkpoints
â”œâ”€â”€ Salidas_Experimentos/                # Exportaciones legacy
â”œâ”€â”€ README.md                            # Referencia en inglÃ©s
â””â”€â”€ README_ES.md                         # Este archivo
```

## ğŸ”§ Detalles TÃ©cnicos

### Componentes del mÃ³dulo NAS

1. **Controller (`controller.py`)**
   - LSTM de 11K parÃ¡metros
   - Genera secuencias ADN
2. **Child Builder (`child_builder.py`)**
   - Construye CNN a partir del ADN
   - Normaliza y valida rangos
3. **REINFORCE (`reinforce.py`)**
   - Gradiente de polÃ­ticas con baseline EMA
   - RegularizaciÃ³n L2 y clipping de gradientes
4. **NAS Trainer (`trainer.py`)**
   - Orquesta el ciclo completo
   - Logging narrativo + checkpoints + reanudaciÃ³n
5. **Utilities (`utils.py`)**
   - Encode/decode ADN
   - ADN aleatorio y representaciones legibles

### Pipeline de entrenamiento

`TrainingPipeline` ofrece:

- Early stopping configurable
- ReduceLROnPlateau y otros schedulers
- Guardado de checkpoints y mÃ©tricas

### Sistema de logging

Niveles jerÃ¡rquicos con Ã­conos:

- ğŸ“‹ INFO, âœ… SUCCESS, ğŸ”¹ STEP, ğŸ“Š METRIC
- âŒ ERROR, ğŸ—ï¸ ARCHITECTURE, ğŸ¯ TRAINING, ğŸ† REWARD

## ğŸ“Š CIFAR-10

- 60,000 imÃ¡genes 32Ã—32 (color)
- 10 clases balanceadas
- 50k train / 10k test
- DesafÃ­os: tamaÃ±o pequeÃ±o, variaciones de vista y luz, occlusiones

## ğŸ“ˆ Resultados

### NASCNN15 (baseline)

| MÃ©trica                | Valor     |
| ----------------------- | --------- |
| ParÃ¡metros             | ~1.9M     |
| Exactitud test          | ~92.5%    |
| Tiempo de entrenamiento | 4-6 horas |

### Salidas de NAS

- `checkpoints/nas/nas_final.pth`
- `checkpoints/nas/best_architecture.json`
- `logs/nas/nas_search_*.log`

## ğŸ§ª Pruebas

Las pruebas automatizadas histÃ³ricas se eliminaron junto con `test_nas_module.py`. Actualmente se recomienda validar mediante:

- Carga de CIFAR-10 (`load.py`)
- Sampleo del Controller (`controller.py`)
- Entrenamiento corto de NASCNN15 (`main.py --mode train`)

## ğŸ“– DocumentaciÃ³n

- `README.md`: Referencia principal en inglÃ©s
- `README_ES.md`: Resumen en espaÃ±ol
- Docstrings detallados en cada mÃ³dulo del paquete `src/`

## ğŸ› ï¸ TecnologÃ­as

- PyTorch 2.0+, TorchVision, NumPy
- Matplotlib/Seaborn, scikit-learn
- Optimizadores: SGD, Adam, RMSprop
- Schedulers: StepLR, ReduceLROnPlateau, OneCycleLR

## ğŸ“ Aplicaciones AcadÃ©micas

Ideal para:

- InvestigaciÃ³n en NAS y AutoML
- Estudios comparativos de arquitecturas
- Cursos avanzados de Deep Learning
- Proyectos de tesis/monografÃ­as

### Para publicaciones

Los logs narrativos facilitan documentar:

- MetodologÃ­a completa
- EvoluciÃ³n de recompensas
- Descubrimiento de arquitecturas

## ğŸ”¬ Extensiones

- Ajustar lÃ­mites ADN en `configs.py`
- Agregar nuevos bloques en `child_builder.py`
- Experimentar con PPO/A2C reemplazando `reinforce.py`

## ğŸ“ Citas

```
@misc{nascnn2025,
  author = {Tschopp, JoaquÃ­n S.},
  title = {Neural Architecture Search with Reinforcement Learning for CIFAR-10},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/tu-usuario/VC-ARN}
}
```

## ğŸ‘¨â€ğŸ’» Autor

**Esp. JoaquÃ­n S. Tschopp**

Especialista en Data Scientist

## ğŸ“„ Licencia

Proyecto bajo licencia MIT.

## ğŸ™ Agradecimientos

- Paper original de Zoph & Le
- Equipo PyTorch
- Creadores del dataset CIFAR-10
- Comunidad open source

## ğŸ”— Recursos

- [Paper original](https://arxiv.org/abs/1611.01578)
- [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)
- [DocumentaciÃ³n PyTorch](https://pytorch.org/docs/)
- [Video REINFORCE](https://www.youtube.com/watch?v=CYUpDogeIL0)

**Estado del proyecto:** ğŸŸ¢ Listo para producciÃ³n
**Ãšltima actualizaciÃ³n:** 22 de noviembre de 2025
